{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Analys The Problem**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Previously on papper \"Facial Expression Recognition Methods in the Wild Based on Fusion Feature of Attention Mechanism and LBP (MDPI Sensors 2023 Q2)\" authors used a model called ResNet-50 combined CBAM and LBP.\n",
    "\n",
    "The accuracy took:\n",
    "- 99.66% on CK+\n",
    "- 74.23% on FER-2013\n",
    "- 89.50 on FER-PLUS \n",
    "- 88.20 on RAF-DB\n",
    "\n",
    "**The problem: What if I change the variant of LBP in this method, does it increase the accuracy spesifically for FER-2013 dataset?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **About Dataset**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The paper used 4 dataset include CK+(59 images), FER-2013 (35,887 images), FER-PLUS(31.412 images), RAF-DB (29,672 images)\n",
    "- CK+ is a controlled dataset and the 3 others datasets are uncontrolled datasets\n",
    "- Controlled dataset has good lightning and pose but uncontrolled datasets got a random of it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Method**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paper's Method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **ResNet-18 + CBAM**\n",
    "\n",
    "    1. Architecture of ResNet-18:\n",
    "    \n",
    "        <img src=\"../src/Structure-of-a-ResNet-18-architecture.png\" alt=\"Windowing of Feature in Faces\" width=\"350\" height=\"250\">\n",
    "\n",
    "    2. CBAM Architecture:\n",
    "\n",
    "        <img src=\"../src/cbam.png\" alt=\"Windowing of Feature in Faces\" width=\"600\" height=\"100\">\n",
    "\n",
    "    3. Authors combine the CBAM module into each block of the ResNet-18 architecture,\n",
    "    \n",
    "        Before and After implement the CBAM module:\n",
    "\n",
    "        <img src=\"../src/oriblock.png\" alt=\"Windowing of Feature in Faces\" width=\"150\" height=\"200\">\n",
    "        <img src=\"../src/blocknCbam.png\" alt=\"Windowing of Feature in Faces\" width=\"150\" height=\"200\">\n",
    "\n",
    "- **Local Binary Patterns (LBP)**\n",
    "\n",
    "    LBP is one of the most generally used texture pattern descriptors for examining local grain features and is regarded as one of the best methods for texture processing, which is widely employed in image processing.\n",
    "\n",
    "- **RCL-Net Model**\n",
    "\n",
    "    After combining all the method such as LBP and ResNet-CBAM, this architecture called by **RCL-Net Model**,\n",
    "\n",
    "    <img src=\"../src/rcl-net.png\" alt=\"Windowing of Feature in Faces\" width=\"700\" height=\"200\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Proposed Method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In planning, switching LBP method in previous architecture with the newest or other variant LBP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **IMPORT LIBRARIES**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Ando File 4 Kuliah\\A SKRIPSI\\RISETku\\KERJA\\CODE\\venv\\Lib\\site-packages\\albumentations\\__init__.py:13: UserWarning: A new version of Albumentations is available: 1.4.18 (you have 1.4.16). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n",
      "  check_for_updates()\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import seaborn as sns\n",
    "\n",
    "# for augmen:\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import torchvision.transforms as T\n",
    "\n",
    "# for mixup augmen:\n",
    "import torch\n",
    "import random\n",
    "\n",
    "# mixup from train.py file\n",
    "import argparse\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# LBP\n",
    "import cv2\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torch.optim as optim\n",
    "import mlflow\n",
    "import mlflow.pytorch\n",
    "\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix\n",
    "from skimage.feature import local_binary_pattern\n",
    "\n",
    "from torchvision.models import resnet18\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **PREPROCESSING**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined_train = pd.read_csv('afterAugTrain.csv')\n",
    "df_public_test = pd.read_csv('afterAugTest.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Visualize datatable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>emotion</th>\n",
       "      <th>usage</th>\n",
       "      <th>resized_pixels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Training</td>\n",
       "      <td>0.53333336 0.5058824 0.4509804 0.39215687 0.34...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>Training</td>\n",
       "      <td>0.09803922 0.09019608 0.07058824 0.06666667 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>Training</td>\n",
       "      <td>0.8980392 0.8862745 0.87058824 0.84705883 0.83...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>Training</td>\n",
       "      <td>0.8627451 0.8745098 0.88235295 0.8980392 0.878...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>Training</td>\n",
       "      <td>0.2784314 0.30980393 0.3647059 0.42352942 0.42...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   emotion     usage                                     resized_pixels\n",
       "0        0  Training  0.53333336 0.5058824 0.4509804 0.39215687 0.34...\n",
       "1        0  Training  0.09803922 0.09019608 0.07058824 0.06666667 0....\n",
       "2        0  Training  0.8980392 0.8862745 0.87058824 0.84705883 0.83...\n",
       "3        0  Training  0.8627451 0.8745098 0.88235295 0.8980392 0.878...\n",
       "4        0  Training  0.2784314 0.30980393 0.3647059 0.42352942 0.42..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tampilan hasil original + augmentasi dataframe setelah normalisasi\n",
    "df_combined_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>emotion</th>\n",
       "      <th>usage</th>\n",
       "      <th>resized_pixels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>PublicTest</td>\n",
       "      <td>0.00390619 0.00390619 0.00390619 0.00390619 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>PublicTest</td>\n",
       "      <td>0.00236832 0.002475971 0.002675894 0.002906574...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>PublicTest</td>\n",
       "      <td>0.00092272204 0.0012302961 0.0016916571 0.0017...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>PublicTest</td>\n",
       "      <td>0.0032602844 0.0031372549 0.0029680892 0.00316...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>PublicTest</td>\n",
       "      <td>0.0013225683 0.0013071896 0.0012610535 0.00121...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   emotion       usage                                     resized_pixels\n",
       "0        0  PublicTest  0.00390619 0.00390619 0.00390619 0.00390619 0....\n",
       "1        1  PublicTest  0.00236832 0.002475971 0.002675894 0.002906574...\n",
       "2        4  PublicTest  0.00092272204 0.0012302961 0.0016916571 0.0017...\n",
       "3        6  PublicTest  0.0032602844 0.0031372549 0.0029680892 0.00316...\n",
       "4        3  PublicTest  0.0013225683 0.0013071896 0.0012610535 0.00121..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tampilan hasil df_public_test dataframe setelah normalisasi\n",
    "df_public_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kolom emotion: Jika ini adalah label kelas, awalnya tipe object maka bisa mengubahnya menjadi tipe data numerik menggunakan LabelEncoder dari scikit-learn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le = LabelEncoder()\n",
    "df_combined_train['emotion'] = le.fit_transform(df_combined_train['emotion'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kolom resized_pixels: Jika kolom ini berisi string yang merepresentasikan array alias saat ini tipe object, maka perlu mengubahnya menjadi array numpy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Konversi string piksel menjadi array NumPy data train\n",
    "df_combined_train['resized_pixels'] = df_combined_train['resized_pixels'].apply(\n",
    "    lambda x: np.fromstring(x, sep=' ').astype(np.float32).reshape(100, 100)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 6104 entries, 0 to 6103\n",
      "Data columns (total 3 columns):\n",
      " #   Column          Non-Null Count  Dtype \n",
      "---  ------          --------------  ----- \n",
      " 0   emotion         6104 non-null   int64 \n",
      " 1   usage           3052 non-null   object\n",
      " 2   resized_pixels  6104 non-null   object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 143.2+ KB\n"
     ]
    }
   ],
   "source": [
    "df_combined_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Konversi string piksel menjadi array NumPy data test\n",
    "df_public_test.loc[:, 'resized_pixels'] = df_public_test['resized_pixels'].apply(\n",
    "    lambda x: np.fromstring(x, sep=' ').astype(np.float32).reshape(100, 100)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# Pastikan semua entri memiliki ukuran 100x100\n",
    "print(df_combined_train['resized_pixels'].apply(lambda x: x.shape == (100, 100)).all())\n",
    "print(df_public_test['resized_pixels'].apply(lambda x: x.shape == (100, 100)).all())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>emotion</th>\n",
       "      <th>usage</th>\n",
       "      <th>resized_pixels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Training</td>\n",
       "      <td>[[0.53333336, 0.5058824, 0.4509804, 0.39215687...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>Training</td>\n",
       "      <td>[[0.09803922, 0.09019608, 0.07058824, 0.066666...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>Training</td>\n",
       "      <td>[[0.8980392, 0.8862745, 0.87058824, 0.84705883...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>Training</td>\n",
       "      <td>[[0.8627451, 0.8745098, 0.88235295, 0.8980392,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>Training</td>\n",
       "      <td>[[0.2784314, 0.30980393, 0.3647059, 0.42352942...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   emotion     usage                                     resized_pixels\n",
       "0        0  Training  [[0.53333336, 0.5058824, 0.4509804, 0.39215687...\n",
       "1        0  Training  [[0.09803922, 0.09019608, 0.07058824, 0.066666...\n",
       "2        0  Training  [[0.8980392, 0.8862745, 0.87058824, 0.84705883...\n",
       "3        0  Training  [[0.8627451, 0.8745098, 0.88235295, 0.8980392,...\n",
       "4        0  Training  [[0.2784314, 0.30980393, 0.3647059, 0.42352942..."
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tampilkan informasi dataframe setelah normalisasi\n",
    "df_combined_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>emotion</th>\n",
       "      <th>usage</th>\n",
       "      <th>resized_pixels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>PublicTest</td>\n",
       "      <td>[[0.00390619, 0.00390619, 0.00390619, 0.003906...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>PublicTest</td>\n",
       "      <td>[[0.00236832, 0.002475971, 0.002675894, 0.0029...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>PublicTest</td>\n",
       "      <td>[[0.00092272204, 0.0012302961, 0.0016916571, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>PublicTest</td>\n",
       "      <td>[[0.0032602844, 0.0031372549, 0.0029680892, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>PublicTest</td>\n",
       "      <td>[[0.0013225683, 0.0013071896, 0.0012610535, 0....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   emotion       usage                                     resized_pixels\n",
       "0        0  PublicTest  [[0.00390619, 0.00390619, 0.00390619, 0.003906...\n",
       "1        1  PublicTest  [[0.00236832, 0.002475971, 0.002675894, 0.0029...\n",
       "2        4  PublicTest  [[0.00092272204, 0.0012302961, 0.0016916571, 0...\n",
       "3        6  PublicTest  [[0.0032602844, 0.0031372549, 0.0029680892, 0....\n",
       "4        3  PublicTest  [[0.0013225683, 0.0013071896, 0.0012610535, 0...."
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tampilkan informasi dataframe setelah normalisasi\n",
    "df_public_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PERCOBAAN MODEL II"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Konversi DataFrame ke Tensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmotionDataset(Dataset):\n",
    "    def __init__(self, dataframe):\n",
    "        self.dataframe = dataframe\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # image = torch.tensor(self.dataframe['resized_pixels'].iloc[idx], dtype=torch.float32).view(1, 100, 100)  # Jika gambar 100x100\n",
    "        # label = torch.tensor(self.dataframe['emotion'].iloc[idx], dtype=torch.long)\n",
    "        image = torch.tensor(self.dataframe['resized_pixels'].iloc[idx], dtype=torch.float32).unsqueeze(0)  # Sudah 100x100, hanya tambah channel dimension\n",
    "        label = torch.tensor(self.dataframe['emotion'].iloc[idx], dtype=torch.long)\n",
    "        return image, label\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Membuat DataLoader:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inisialisasi dataset\n",
    "train_dataset = EmotionDataset(df_combined_train)\n",
    "test_dataset = EmotionDataset(df_public_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Inisialisasi DataLoader\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=False)\n",
    "val_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "# train_loader = DataLoader(train_dataset, batch_size=64, shuffle=False, num_workers=4, pin_memory=True)\n",
    "# test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, num_workers=4, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()  # Bersihkan cache CUDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Channel Attention Module\n",
    "class ChannelAttention(nn.Module):\n",
    "    def __init__(self, in_planes, ratio=16):\n",
    "        super(ChannelAttention, self).__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.max_pool = nn.AdaptiveMaxPool2d(1)\n",
    "\n",
    "        self.fc1 = nn.Conv2d(in_planes, in_planes // ratio, 1, bias=False)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2 = nn.Conv2d(in_planes // ratio, in_planes, 1, bias=False)\n",
    "\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        avg_out = self.fc2(self.relu1(self.fc1(self.avg_pool(x))))\n",
    "        max_out = self.fc2(self.relu1(self.fc1(self.max_pool(x))))\n",
    "        out = avg_out + max_out\n",
    "        return self.sigmoid(out)\n",
    "\n",
    "\n",
    "# Spatial Attention Module\n",
    "class SpatialAttention(nn.Module):\n",
    "    def __init__(self, kernel_size=7):\n",
    "        super(SpatialAttention, self).__init__()\n",
    "        assert kernel_size in (3, 7), \"kernel size must be 3 or 7\"\n",
    "        padding = 3 if kernel_size == 7 else 1\n",
    "\n",
    "        self.conv1 = nn.Conv2d(2, 1, kernel_size, padding=padding, bias=False)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        avg_out = torch.mean(x, dim=1, keepdim=True)\n",
    "        max_out, _ = torch.max(x, dim=1, keepdim=True)\n",
    "        x = torch.cat([avg_out, max_out], dim=1)\n",
    "        x = self.conv1(x)\n",
    "        return self.sigmoid(x)\n",
    "\n",
    "\n",
    "# CBAM Block\n",
    "class CBAM(nn.Module):\n",
    "    def __init__(self, in_planes, ratio=16, kernel_size=7):\n",
    "        super(CBAM, self).__init__()\n",
    "        self.ca = ChannelAttention(in_planes, ratio)\n",
    "        self.sa = SpatialAttention(kernel_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.ca(x) * x\n",
    "        out = self.sa(out) * out\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Ando File 4 Kuliah\\A SKRIPSI\\RISETku\\KERJA\\CODE\\venv\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "d:\\Ando File 4 Kuliah\\A SKRIPSI\\RISETku\\KERJA\\CODE\\venv\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet18_CBAM_7Classes(\n",
      "  (conv1): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): Sequential(\n",
      "      (0): BasicBlock(\n",
      "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (1): BasicBlock(\n",
      "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): CBAM(\n",
      "      (ca): ChannelAttention(\n",
      "        (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
      "        (max_pool): AdaptiveMaxPool2d(output_size=1)\n",
      "        (fc1): Conv2d(64, 4, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (relu1): ReLU()\n",
      "        (fc2): Conv2d(4, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (sigmoid): Sigmoid()\n",
      "      )\n",
      "      (sa): SpatialAttention(\n",
      "        (conv1): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)\n",
      "        (sigmoid): Sigmoid()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): Sequential(\n",
      "      (0): BasicBlock(\n",
      "        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): BasicBlock(\n",
      "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): CBAM(\n",
      "      (ca): ChannelAttention(\n",
      "        (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
      "        (max_pool): AdaptiveMaxPool2d(output_size=1)\n",
      "        (fc1): Conv2d(128, 8, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (relu1): ReLU()\n",
      "        (fc2): Conv2d(8, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (sigmoid): Sigmoid()\n",
      "      )\n",
      "      (sa): SpatialAttention(\n",
      "        (conv1): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)\n",
      "        (sigmoid): Sigmoid()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): Sequential(\n",
      "      (0): BasicBlock(\n",
      "        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): BasicBlock(\n",
      "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): CBAM(\n",
      "      (ca): ChannelAttention(\n",
      "        (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
      "        (max_pool): AdaptiveMaxPool2d(output_size=1)\n",
      "        (fc1): Conv2d(256, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (relu1): ReLU()\n",
      "        (fc2): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (sigmoid): Sigmoid()\n",
      "      )\n",
      "      (sa): SpatialAttention(\n",
      "        (conv1): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)\n",
      "        (sigmoid): Sigmoid()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): Sequential(\n",
      "      (0): BasicBlock(\n",
      "        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): BasicBlock(\n",
      "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): CBAM(\n",
      "      (ca): ChannelAttention(\n",
      "        (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
      "        (max_pool): AdaptiveMaxPool2d(output_size=1)\n",
      "        (fc1): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (relu1): ReLU()\n",
      "        (fc2): Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (sigmoid): Sigmoid()\n",
      "      )\n",
      "      (sa): SpatialAttention(\n",
      "        (conv1): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)\n",
      "        (sigmoid): Sigmoid()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc): Linear(in_features=512, out_features=512, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Load the pre-trained ResNet18 model\n",
    "model = resnet18(pretrained=True)\n",
    "\n",
    "# Modify the ResNet18 model by adding CBAM after conv2 to conv5\n",
    "class ResNet18_CBAM_7Classes(nn.Module):\n",
    "    def __init__(self, original_model, num_classes=7):\n",
    "        super(ResNet18_CBAM_7Classes, self).__init__()\n",
    "        \n",
    "        # Take the original model's layers\n",
    "        self.conv1 = nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1, bias=False) \n",
    "        self.bn1 = original_model.bn1\n",
    "        self.relu = original_model.relu\n",
    "        self.maxpool = original_model.maxpool\n",
    "        \n",
    "        # Add CBAM after each convolutional block\n",
    "        self.layer1 = nn.Sequential(\n",
    "            original_model.layer1,\n",
    "            CBAM(64)  # After conv2\n",
    "        )\n",
    "        self.layer2 = nn.Sequential(\n",
    "            original_model.layer2,\n",
    "            CBAM(128)  # After conv3\n",
    "        )\n",
    "        self.layer3 = nn.Sequential(\n",
    "            original_model.layer3,\n",
    "            CBAM(256)  # After conv4\n",
    "        )\n",
    "        self.layer4 = nn.Sequential(\n",
    "            original_model.layer4,\n",
    "            CBAM(512)  # After conv5\n",
    "        )\n",
    "        \n",
    "        # Original avgpool layer\n",
    "        self.avgpool = original_model.avgpool\n",
    "        \n",
    "        # Modify the FC layer to output 7 classes\n",
    "        self.fc = nn.Linear(512, 512)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Forward pass through the modified network\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "        \n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        \n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "# Create the modified ResNet18 with CBAM for 7 classes\n",
    "model = ResNet18_CBAM_7Classes(model, num_classes=7)\n",
    "\n",
    "\n",
    "print(model.eval())  # Seharusnya [1, 7]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 2, 48, 48])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def lbp_transform(image):\n",
    "    \"\"\"\n",
    "    Apply the LBP operation on the input image (assumed to be grayscale).\n",
    "    The image tensor should be of shape [batch_size, 1, height, width].\n",
    "    \n",
    "    :param image: Input image tensor (on GPU)\n",
    "    :return: LBP transformed image tensor\n",
    "    \"\"\"\n",
    "    # Define the LBP kernel (neighboring comparison)\n",
    "    lbp_kernel = torch.tensor([[[[ 1,  1,  1],\n",
    "                                 [ 1,  0, -1],\n",
    "                                 [-1, -1, -1]]],\n",
    "                               [[[ 1,  1,  1],\n",
    "                                 [ 1,  0, -1],\n",
    "                                 [-1, -1, -1]]]], dtype=torch.float32, device=image.device)\n",
    "\n",
    "    # Apply the kernel using convolution (with padding to keep the same size)\n",
    "    lbp_image = F.conv2d(image, lbp_kernel, padding=1)\n",
    "\n",
    "    # Convert the convolution result into binary (using sign function)\n",
    "    lbp_image = (lbp_image > 0).float()\n",
    "\n",
    "    return lbp_image\n",
    "\n",
    "# Example of using the function on GPU\n",
    "# Assume `input_image` is a grayscale image tensor of shape [batch_size, 1, height, width]\n",
    "input_image = torch.randn((8, 1, 48, 48), device='cuda')  # Example tensor on GPU\n",
    "lbp_output = lbp_transform(input_image)\n",
    "\n",
    "print(lbp_output.shape)  # Should be the same as input shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RCL_Net(\n",
      "  (resnet_cbam): ResNet18_CBAM_7Classes(\n",
      "    (conv1): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "    (layer1): Sequential(\n",
      "      (0): Sequential(\n",
      "        (0): BasicBlock(\n",
      "          (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (1): BasicBlock(\n",
      "          (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): CBAM(\n",
      "        (ca): ChannelAttention(\n",
      "          (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
      "          (max_pool): AdaptiveMaxPool2d(output_size=1)\n",
      "          (fc1): Conv2d(64, 4, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (relu1): ReLU()\n",
      "          (fc2): Conv2d(4, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (sigmoid): Sigmoid()\n",
      "        )\n",
      "        (sa): SpatialAttention(\n",
      "          (conv1): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)\n",
      "          (sigmoid): Sigmoid()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (layer2): Sequential(\n",
      "      (0): Sequential(\n",
      "        (0): BasicBlock(\n",
      "          (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (downsample): Sequential(\n",
      "            (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (1): BasicBlock(\n",
      "          (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): CBAM(\n",
      "        (ca): ChannelAttention(\n",
      "          (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
      "          (max_pool): AdaptiveMaxPool2d(output_size=1)\n",
      "          (fc1): Conv2d(128, 8, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (relu1): ReLU()\n",
      "          (fc2): Conv2d(8, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (sigmoid): Sigmoid()\n",
      "        )\n",
      "        (sa): SpatialAttention(\n",
      "          (conv1): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)\n",
      "          (sigmoid): Sigmoid()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (layer3): Sequential(\n",
      "      (0): Sequential(\n",
      "        (0): BasicBlock(\n",
      "          (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (downsample): Sequential(\n",
      "            (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (1): BasicBlock(\n",
      "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): CBAM(\n",
      "        (ca): ChannelAttention(\n",
      "          (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
      "          (max_pool): AdaptiveMaxPool2d(output_size=1)\n",
      "          (fc1): Conv2d(256, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (relu1): ReLU()\n",
      "          (fc2): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (sigmoid): Sigmoid()\n",
      "        )\n",
      "        (sa): SpatialAttention(\n",
      "          (conv1): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)\n",
      "          (sigmoid): Sigmoid()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (layer4): Sequential(\n",
      "      (0): Sequential(\n",
      "        (0): BasicBlock(\n",
      "          (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (downsample): Sequential(\n",
      "            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (1): BasicBlock(\n",
      "          (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): CBAM(\n",
      "        (ca): ChannelAttention(\n",
      "          (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
      "          (max_pool): AdaptiveMaxPool2d(output_size=1)\n",
      "          (fc1): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (relu1): ReLU()\n",
      "          (fc2): Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (sigmoid): Sigmoid()\n",
      "        )\n",
      "        (sa): SpatialAttention(\n",
      "          (conv1): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)\n",
      "          (sigmoid): Sigmoid()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "    (fc): Linear(in_features=512, out_features=512, bias=True)\n",
      "  )\n",
      "  (conv1_lbp): Conv2d(2, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv2_lbp): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv3_lbp): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (pool_lbp): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc_lbp): Linear(in_features=256, out_features=256, bias=True)\n",
      "  (fc_resnet): Linear(in_features=512, out_features=256, bias=True)\n",
      "  (fc_fusion): Linear(in_features=512, out_features=14, bias=True)\n",
      "  (fc_output): Linear(in_features=14, out_features=7, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision.models import resnet18\n",
    "\n",
    "# LBP feature extraction function (you can implement this with OpenCV or any other method)\n",
    "# def lbp_transform(image):\n",
    "#     # Placeholder: Implement LBP transformation here\n",
    "#     # For now, assume it returns a tensor of the same size as the input image\n",
    "#     return image  # This should be the LBP transformed image\n",
    "\n",
    "# Define the modified model class with the LBP branch\n",
    "class RCL_Net(nn.Module):\n",
    "    def __init__(self, original_model, num_classes=7):\n",
    "        super(RCL_Net, self).__init__()\n",
    "        \n",
    "        # ResNet-CBAM branch (global feature extraction)\n",
    "        self.resnet_cbam = ResNet18_CBAM_7Classes(original_model, num_classes)\n",
    "        \n",
    "        # LBP branch (local feature extraction)\n",
    "        self.conv1_lbp = nn.Conv2d(2, 64, kernel_size=3, stride=1, padding=1)  # Input: 2 channels\n",
    "        self.conv2_lbp = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv3_lbp = nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool_lbp = nn.AdaptiveAvgPool2d((1, 1))  # Pooling to [batch_size, 256, 1, 1]\n",
    "        \n",
    "        # Fully connected layer for LBP branch (FC2)\n",
    "        self.fc_lbp = nn.Linear(256, 256)\n",
    "        \n",
    "        # Fully connected layers for ResNet-CBAM branch (FC1)\n",
    "        self.fc_resnet = nn.Linear(512, 256)  # FC1 for ResNet-CBAM\n",
    "        \n",
    "        # Fully connected layers after feature fusion (FC3)\n",
    "        self.fc_fusion = nn.Linear(256 + 256, 14)  # FC3\n",
    "        self.fc_output = nn.Linear(14, num_classes)  # Final classification\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Pass through ResNet-CBAM branch (global features)\n",
    "        x_resnet = self.resnet_cbam(x)  # Output: [batch_size, 512]\n",
    "        x_resnet = F.relu(self.fc_resnet(x_resnet))  # Shape: [batch_size, 256] (FC1)\n",
    "\n",
    "        # LBP branch (local features)\n",
    "        lbp_image = lbp_transform(x)  # Apply LBP transformation\n",
    "        lbp_x = F.relu(self.conv1_lbp(lbp_image))\n",
    "        lbp_x = F.relu(self.conv2_lbp(lbp_x))\n",
    "        lbp_x = F.relu(self.conv3_lbp(lbp_x))\n",
    "        lbp_x = self.pool_lbp(lbp_x)  # Pool to [batch_size, 256, 1, 1]\n",
    "        lbp_x = lbp_x.view(lbp_x.size(0), -1)  # Flatten to [batch_size, 256]\n",
    "        lbp_x = F.relu(self.fc_lbp(lbp_x))  # Shape: [batch_size, 256] (FC2)\n",
    "\n",
    "        # Concatenate ResNet-CBAM and LBP feature vectors\n",
    "        fused = torch.cat((x_resnet, lbp_x), dim=1)  # Shape: [batch_size, 256 + 256]\n",
    "\n",
    "        # Pass through fusion fully connected layers (FC3)\n",
    "        fused = F.sigmoid(self.fc_fusion(fused))  # Apply sigmoid activation ()\n",
    "        \n",
    "        # Final classification output\n",
    "        output = self.fc_output(fused)  # Output: [batch_size, num_classes]\n",
    "        \n",
    "        return output\n",
    "\n",
    "# Instantiate the model with ResNet18 backbone\n",
    "original_model = resnet18(pretrained=True)\n",
    "model = RCL_Net(original_model, num_classes=7)\n",
    "\n",
    "print(model.eval())  # To check the model architecture\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mixup_data(x, y, alpha=1.0):\n",
    "    \"\"\"Mixup data.\"\"\"\n",
    "    if alpha > 0:\n",
    "        lam = np.random.beta(alpha, alpha)\n",
    "    else:\n",
    "        lam = 1\n",
    "    batch_size = x.size()[0]\n",
    "    index = torch.randperm(batch_size).to(x.device)\n",
    "\n",
    "    mixed_x = lam * x + (1 - lam) * x[index, :]\n",
    "    y_a, y_b = y, y[index]\n",
    "    return mixed_x, y_a, y_b, lam\n",
    "\n",
    "def mixup_criterion(criterion, pred, y_a, y_b, lam):\n",
    "    return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def train_and_evaluate(model, train_loader, val_loader, \n",
    "                      num_epochs=300,\n",
    "                      lr=0.001,  # Learning rate yang lebih kecil\n",
    "                      momentum=0.1,  # Momentum yang lebih besar\n",
    "                      weight_decay=0.0001,\n",
    "                      max_early_stop=30,\n",
    "                      save_path='./checkpoints',\n",
    "                      resume=False):\n",
    "    \n",
    "    # device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    device = torch.device(\"cpu\")\n",
    "    model = model.to(device)\n",
    "    \n",
    "    optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum, weight_decay=weight_decay)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # Tambahkan learning rate scheduler\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', \n",
    "                                                    factor=0.1, patience=5, \n",
    "                                                    verbose=True)\n",
    "    \n",
    "    best_val_acc = 0.0\n",
    "    start_epoch = 0\n",
    "    early_stop_count = 0\n",
    "\n",
    "    # Resume dari checkpoint jika ada\n",
    "    if resume and os.path.exists(os.path.join(save_path, 'best_model.pth')):\n",
    "        checkpoint = torch.load(os.path.join(save_path, 'best_model.pth'))\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "        start_epoch = checkpoint['epoch']\n",
    "        best_val_acc = checkpoint['best_val_acc']\n",
    "        print(f\"Resuming training from epoch {start_epoch + 1} with best validation accuracy {best_val_acc:.4f}\")\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(start_epoch, num_epochs):\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        # Training progress bar\n",
    "        train_progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch + 1}/{num_epochs} (Training)\", leave=True)\n",
    "        \n",
    "        for inputs, labels in train_progress_bar:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # Backward pass dengan gradient clipping\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Perhitungan metrik training\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "            \n",
    "            # Update progress bar\n",
    "            train_acc = correct / total\n",
    "            train_progress_bar.set_postfix(acc=f\"{train_acc:.3f}\", loss=f\"{loss.item():.3f}\")\n",
    "        \n",
    "        train_loss = running_loss / len(train_loader)\n",
    "        train_acc = correct / total\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        all_predictions = []\n",
    "        all_labels = []\n",
    "        \n",
    "        val_progress_bar = tqdm(val_loader, desc=f\"Epoch {epoch + 1}/{num_epochs} (Validation)\", leave=False)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_progress_bar:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                \n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                \n",
    "                val_loss += loss.item()\n",
    "                _, predicted = outputs.max(1)\n",
    "                val_total += labels.size(0)\n",
    "                val_correct += predicted.eq(labels).sum().item()\n",
    "                \n",
    "                # Simpan predictions dan labels untuk metrik\n",
    "                all_predictions.extend(predicted.cpu().numpy())\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        # Hitung metrik validasi\n",
    "        val_loss = val_loss / len(val_loader)\n",
    "        val_acc = val_correct / val_total\n",
    "        \n",
    "        # Hitung precision, recall, dan F1 score\n",
    "        precision = precision_score(all_labels, all_predictions, average='macro', zero_division=0)\n",
    "        recall = recall_score(all_labels, all_predictions, average='macro', zero_division=0)\n",
    "        f1 = f1_score(all_labels, all_predictions, average='macro', zero_division=0)\n",
    "        \n",
    "        # Update learning rate scheduler\n",
    "        scheduler.step(val_acc)\n",
    "        \n",
    "        # Print metrics\n",
    "        print(f\"\\nEpoch [{epoch + 1}/{num_epochs}]\")\n",
    "        print(f\"Train Loss: {train_loss:.4f}, Train Accuracy: {train_acc:.4f}, hitung : {correct}/{total}\")\n",
    "        print(f\"Val Loss: {val_loss:.4f}, Val Accuracy: {val_acc:.4f}\")\n",
    "        print(f\"Precision: {precision:.4f}, Recall: {recall:.4f}, F1: {f1:.4f}\")\n",
    "        print(f\"Current LR: {optimizer.param_groups[0]['lr']:.6f}\\n\")\n",
    "        \n",
    "        # Save best model\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            best_checkpoint_path = os.path.join(save_path, 'best_model.pth')\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'scheduler_state_dict': scheduler.state_dict(),\n",
    "                'best_val_acc': best_val_acc,\n",
    "                'train_loss': train_loss,\n",
    "                'val_loss': val_loss,\n",
    "                'precision': precision,\n",
    "                'recall': recall,\n",
    "                'f1': f1\n",
    "            }, best_checkpoint_path)\n",
    "            print(f\"Best model saved at epoch {epoch + 1} with validation accuracy {val_acc:.4f}\")\n",
    "            early_stop_count = 0\n",
    "        else:\n",
    "            early_stop_count += 1\n",
    "            print(f\"Early stop count: {early_stop_count}/{max_early_stop}\")\n",
    "        \n",
    "        # Save periodic checkpoint\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            checkpoint_path = os.path.join(save_path, f'checkpoint_epoch_{epoch + 1}.pth')\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'scheduler_state_dict': scheduler.state_dict(),\n",
    "                'best_val_acc': best_val_acc\n",
    "            }, checkpoint_path)\n",
    "            print(f\"Checkpoint saved at epoch {epoch + 1}\")\n",
    "        \n",
    "        # Early stopping check\n",
    "        if early_stop_count >= max_early_stop:\n",
    "            print(f\"Training stopped early at epoch {epoch + 1}\")\n",
    "            break\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Ando File 4 Kuliah\\A SKRIPSI\\RISETku\\KERJA\\CODE\\venv\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/300 (Training): 100%|| 96/96 [27:25<00:00, 17.14s/it, acc=0.255, loss=0.014] \n",
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [1/300]\n",
      "Train Loss: 5.8688, Train Accuracy: 0.2549, hitung : 1556/6104\n",
      "Val Loss: 10.2062, Val Accuracy: 0.1691\n",
      "Precision: 0.0242, Recall: 0.1429, F1: 0.0413\n",
      "Current LR: 0.900000\n",
      "Best model saved at epoch 1 with validation accuracy 0.1691\n",
      "Epoch 2/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/300 (Training): 100%|| 96/96 [28:06<00:00, 17.57s/it, acc=0.108, loss=0.021] \n",
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [2/300]\n",
      "Train Loss: 7.4432, Train Accuracy: 0.1081, hitung : 660/6104\n",
      "Val Loss: 9.7888, Val Accuracy: 0.1691\n",
      "Precision: 0.0242, Recall: 0.1429, F1: 0.0413\n",
      "Current LR: 0.900000\n",
      "Early stop count: 1/30\n",
      "Epoch 3/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/300 (Training): 100%|| 96/96 [30:08<00:00, 18.84s/it, acc=0.119, loss=0.018] \n",
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [3/300]\n",
      "Train Loss: 7.4340, Train Accuracy: 0.1186, hitung : 724/6104\n",
      "Val Loss: 9.8287, Val Accuracy: 0.1691\n",
      "Precision: 0.0242, Recall: 0.1429, F1: 0.0413\n",
      "Current LR: 0.900000\n",
      "Early stop count: 2/30\n",
      "Epoch 4/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/300 (Training): 100%|| 96/96 [25:41<00:00, 16.06s/it, acc=0.119, loss=0.022] \n",
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [4/300]\n",
      "Train Loss: 7.4516, Train Accuracy: 0.1186, hitung : 724/6104\n",
      "Val Loss: 9.7359, Val Accuracy: 0.1691\n",
      "Precision: 0.0242, Recall: 0.1429, F1: 0.0413\n",
      "Current LR: 0.900000\n",
      "Early stop count: 3/30\n",
      "Epoch 5/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/300 (Training):   5%|         | 5/96 [01:16<23:19, 15.38s/it, acc=0.000, loss=6.943] \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m os\u001b[38;5;241m.\u001b[39mmakedirs(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./checkpoints\u001b[39m\u001b[38;5;124m'\u001b[39m, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Training\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_and_evaluate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m300\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.9\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmomentum\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.8\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.00001\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_early_stop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m30\u001b[39;49m\n\u001b[0;32m     14\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[22], line 64\u001b[0m, in \u001b[0;36mtrain_and_evaluate\u001b[1;34m(model, train_loader, val_loader, num_epochs, lr, momentum, weight_decay, max_early_stop, save_path, resume)\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;66;03m# Backward pass dengan gradient clipping\u001b[39;00m\n\u001b[0;32m     63\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 64\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     65\u001b[0m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(model\u001b[38;5;241m.\u001b[39mparameters(), max_norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m)\n\u001b[0;32m     66\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[1;32md:\\Ando File 4 Kuliah\\A SKRIPSI\\RISETku\\KERJA\\CODE\\venv\\Lib\\site-packages\\torch\\_tensor.py:521\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    512\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    513\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    514\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    519\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    520\u001b[0m     )\n\u001b[1;32m--> 521\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    522\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    523\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Ando File 4 Kuliah\\A SKRIPSI\\RISETku\\KERJA\\CODE\\venv\\Lib\\site-packages\\torch\\autograd\\__init__.py:289\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    284\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    286\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    287\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    288\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 289\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    290\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    291\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    292\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    293\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    297\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Ando File 4 Kuliah\\A SKRIPSI\\RISETku\\KERJA\\CODE\\venv\\Lib\\site-packages\\torch\\autograd\\graph.py:769\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    767\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    768\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 769\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    770\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    771\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    772\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    773\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "# Buat directory untuk checkpoints\n",
    "os.makedirs('./checkpoints', exist_ok=True)\n",
    "\n",
    "# Training\n",
    "model = train_and_evaluate(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    num_epochs=300,\n",
    "    lr=0.9,\n",
    "    momentum=0.8,\n",
    "    weight_decay=0.00001,\n",
    "    max_early_stop=30\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
